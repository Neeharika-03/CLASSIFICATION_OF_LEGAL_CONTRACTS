{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a59f50-4eaa-45b9-b197-34b67a386921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 766\n",
      "Dev size: 86\n",
      "Test size: 95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df= pd.read_csv(\"preprocessed_data.csv\")\n",
    "df.groupby('tag').describe()\n",
    "labels=[]\n",
    "for i in range(len(df)):\n",
    "    if('obligation'in df['tag'][i]):\n",
    "        labels.append('yes')\n",
    "    else:\n",
    "        labels.append('no')\n",
    "rest_texts, test_texts, rest_labels, test_labels = train_test_split(df['sentence'], labels, test_size=0.1, random_state=1)\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
    "\n",
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Dev size:\", len(dev_texts))\n",
    "print(\"Test size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c57279-f63b-4386-bbc5-dff832327d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0, 'no': 1}\n"
     ]
    }
   ],
   "source": [
    "target_names = list(set(labels))\n",
    "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2470568-226e-461c-9841-07e0b44f610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Baseline accuracy: 0.8210526315789474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "parameters = {'lr__C': [0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
    "\n",
    "best_classifier = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n",
    "best_classifier.fit(train_texts, train_labels)\n",
    "best_predictions = best_classifier.predict(test_texts)\n",
    "\n",
    "baseline_accuracy = np.mean(best_predictions == test_labels)\n",
    "print(\"Baseline accuracy:\", baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6775c57-70fc-434c-a960-d5268740750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d21275fd-90e9-480d-8af7-4510a640335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1138b9c9-94fc-42eb-a4fa-16ef908f9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = len(label2idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3aae3a6-b841-4d48-a210-c4963ac0bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_SEQ_LENGTH=100\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label2idx[label]\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381e4c3a-00f0-4b5d-936f-0110eaae34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68cc820c-4728-426e-9751-2bccee9a4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, attention_mask=input_mask,\n",
    "                                          token_type_ids=segment_ids, labels=label_ids).loss\n",
    "            logits= model(input_ids, attention_mask=input_mask,\n",
    "                                          token_type_ids=segment_ids, labels=label_ids).logits\n",
    "            print(tmp_eval_loss)\n",
    "\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += outputs\n",
    "        correct_labels += list(label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels , predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c284783-1660-48a4-9df0-383895e84a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_GRAD_NORM = 5\n",
    "\n",
    "num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d0c53ed-202c-48df-993a-83c347d15825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                             | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_8456/1537822491.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae21abd540e4abba96cf5ae5aea5647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8456/3885943829.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888399bf0dc24c6c87b97b001c6c93fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8959e-05, device='cuda:0')\n",
      "tensor(0.0209, device='cuda:0')\n",
      "tensor(0.7090, device='cuda:0')\n",
      "tensor(0.7518, device='cuda:0')\n",
      "tensor(8.2180e-06, device='cuda:0')\n",
      "tensor(7.8479e-06, device='cuda:0')\n",
      "Loss history: []\n",
      "Dev loss: 0.24694426167661732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|█▊                                   | 1/20 [00:05<01:49,  5.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbd42375dea4bcdba372adbb90be56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6e7924b7e64492ae5b83b1101bdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6136e-05, device='cuda:0')\n",
      "tensor(0.0280, device='cuda:0')\n",
      "tensor(0.7888, device='cuda:0')\n",
      "tensor(0.8261, device='cuda:0')\n",
      "tensor(2.4214e-06, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▋                                 | 2/20 [00:09<01:27,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3246e-06, device='cuda:0')\n",
      "Loss history: [0.24694426167661732]\n",
      "Dev loss: 0.2738210252045216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a502b68b2f1b47ecb5dcb7224d7535a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c14df187334bbaa00d84a49814b654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9682e-05, device='cuda:0')\n",
      "tensor(0.0063, device='cuda:0')\n",
      "tensor(0.8231, device='cuda:0')\n",
      "tensor(0.8588, device='cuda:0')\n",
      "tensor(1.4082e-06, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▋                                 | 2/20 [00:14<02:07,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3312e-06, device='cuda:0')\n",
      "Loss history: [0.24694426167661732, 0.2738210252045216]\n",
      "Dev loss: 0.28136535717279304\n",
      "No improvement on development set. Finish training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "OUTPUT_DIR = \"/tmp/\"\n",
    "MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "PATIENCE = 2\n",
    "\n",
    "loss_history = []\n",
    "no_improvement = 0\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "    dev_loss, _, _ = evaluate(model, dev_dataloader)\n",
    "    \n",
    "    print(\"Loss history:\", loss_history)\n",
    "    print(\"Dev loss:\", dev_loss)\n",
    "    \n",
    "    if len(loss_history) == 0 : #or dev_loss < min(loss_history):\n",
    "        no_improvement = 0\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    \n",
    "    if no_improvement >= PATIENCE: \n",
    "        print(\"No improvement on development set. Finish training.\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    loss_history.append(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "258555ed-bb52-4493-b032-455ad87a8d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8456/3885943829.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d5211f6d1a48a8814d216dd53aab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7858e-06, device='cuda:0')\n",
      "tensor(7.8082e-06, device='cuda:0')\n",
      "tensor(7.8380e-06, device='cuda:0')\n",
      "tensor(7.8678e-06, device='cuda:0')\n",
      "tensor(8.2925e-06, device='cuda:0')\n",
      "tensor(8.4638e-06, device='cuda:0')\n",
      "tensor(7.5325e-06, device='cuda:0')\n",
      "tensor(7.2643e-06, device='cuda:0')\n",
      "tensor(8.7767e-06, device='cuda:0')\n",
      "tensor(7.4431e-06, device='cuda:0')\n",
      "tensor(7.9721e-06, device='cuda:0')\n",
      "tensor(7.7933e-06, device='cuda:0')\n",
      "tensor(7.2568e-06, device='cuda:0')\n",
      "tensor(8.4191e-06, device='cuda:0')\n",
      "tensor(9.0822e-06, device='cuda:0')\n",
      "tensor(7.5996e-06, device='cuda:0')\n",
      "tensor(8.1360e-06, device='cuda:0')\n",
      "tensor(7.9348e-06, device='cuda:0')\n",
      "tensor(8.1285e-06, device='cuda:0')\n",
      "tensor(8.4191e-06, device='cuda:0')\n",
      "tensor(7.6443e-06, device='cuda:0')\n",
      "tensor(8.7171e-06, device='cuda:0')\n",
      "tensor(8.3297e-06, device='cuda:0')\n",
      "tensor(8.0838e-06, device='cuda:0')\n",
      "tensor(8.3297e-06, device='cuda:0')\n",
      "tensor(8.6650e-06, device='cuda:0')\n",
      "tensor(7.7262e-06, device='cuda:0')\n",
      "tensor(8.4415e-06, device='cuda:0')\n",
      "tensor(9.3728e-06, device='cuda:0')\n",
      "tensor(9.1642e-06, device='cuda:0')\n",
      "tensor(7.7262e-06, device='cuda:0')\n",
      "tensor(8.5532e-06, device='cuda:0')\n",
      "tensor(8.7842e-06, device='cuda:0')\n",
      "tensor(9.4249e-06, device='cuda:0')\n",
      "tensor(9.1716e-06, device='cuda:0')\n",
      "tensor(7.1153e-06, device='cuda:0')\n",
      "tensor(8.1062e-06, device='cuda:0')\n",
      "tensor(7.0408e-06, device='cuda:0')\n",
      "tensor(7.8231e-06, device='cuda:0')\n",
      "tensor(7.1600e-06, device='cuda:0')\n",
      "tensor(9.0301e-06, device='cuda:0')\n",
      "tensor(8.1583e-06, device='cuda:0')\n",
      "tensor(8.7097e-06, device='cuda:0')\n",
      "tensor(8.8960e-06, device='cuda:0')\n",
      "tensor(7.4729e-06, device='cuda:0')\n",
      "tensor(9.2238e-06, device='cuda:0')\n",
      "tensor(8.1285e-06, device='cuda:0')\n",
      "tensor(9.1535e-06, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5881e3cac8224b099f8e778b137039d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8959e-05, device='cuda:0')\n",
      "tensor(0.0209, device='cuda:0')\n",
      "tensor(0.7090, device='cuda:0')\n",
      "tensor(0.7518, device='cuda:0')\n",
      "tensor(8.2180e-06, device='cuda:0')\n",
      "tensor(7.8479e-06, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f693cfb2c5684c2ca606995456325c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4487, device='cuda:0')\n",
      "tensor(2.2076, device='cuda:0')\n",
      "tensor(2.1724e-05, device='cuda:0')\n",
      "tensor(0.6833, device='cuda:0')\n",
      "tensor(8.6054e-06, device='cuda:0')\n",
      "tensor(7.5658e-06, device='cuda:0')\n",
      "Training performance: (1.0, 1.0, 1.0, None)\n",
      "Development performance: (0.9767441860465116, 0.9767441860465116, 0.9767441860465116, None)\n",
      "Test performance: (0.9368421052631579, 0.9368421052631579, 0.9368421052631579, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.92      0.92      0.92        37\n",
      "          no       0.95      0.95      0.95        58\n",
      "\n",
      "    accuracy                           0.94        95\n",
      "   macro avg       0.93      0.93      0.93        95\n",
      "weighted avg       0.94      0.94      0.94        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc: storage)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = len(target_names))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "_, train_correct, train_predicted = evaluate(model, train_dataloader)\n",
    "_, dev_correct, dev_predicted = evaluate(model, dev_dataloader)\n",
    "_, test_correct, test_predicted = evaluate(model, test_dataloader)\n",
    "\n",
    "print(\"Training performance:\", precision_recall_fscore_support(train_correct, train_predicted, average=\"micro\"))\n",
    "print(\"Development performance:\", precision_recall_fscore_support(dev_correct, dev_predicted, average=\"micro\"))\n",
    "print(\"Test performance:\", precision_recall_fscore_support(test_correct, test_predicted, average=\"micro\"))\n",
    "\n",
    "bert_accuracy = np.mean(test_predicted == test_correct)\n",
    "\n",
    "print(classification_report(test_correct, test_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d52f835-8605-452b-8e55-b2aa7d4647b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-4.9479,  4.2061],\n",
       "        [ 4.2147, -4.0075],\n",
       "        [-5.0396,  4.2019],\n",
       "        [ 4.2294, -4.0396],\n",
       "        [-5.0102,  4.1854],\n",
       "        [-5.0054,  4.1997],\n",
       "        [ 4.2364, -4.0481],\n",
       "        [-4.9906,  4.2057],\n",
       "        [ 4.2229, -4.0297],\n",
       "        [ 4.2240, -4.0349],\n",
       "        [-5.0509,  4.1822],\n",
       "        [-4.9787,  4.1814],\n",
       "        [-5.0388,  4.1931],\n",
       "        [-5.0260,  4.1876]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids, attention_mask=input_mask,\n",
    "                                          token_type_ids=segment_ids, labels=label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8436b-c676-4094-980a-e796855ae486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
